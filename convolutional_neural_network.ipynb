{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuva04/ML_Beginners/blob/main/convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "- an image is inputted, and the output is a label\n",
        "\n",
        "## Step-1: Convolution operation\n",
        "- the image is represented in terms of a 2D or 3D array of values from 0 to 255\n",
        "- another matrix called the feature detector (also called kernel or filter) is used, which is usually a 3x3 matrix, but can take up any other value as well depending on the use case\n",
        "- the 2 arrays are convolved with each other\n",
        "- the result is called a feature map (or activation map)\n",
        "- this process reduces the size of the image\n",
        "- this process ofcourse leads to losing some of the information, but it makes some features more distinguishable and hence easier to detect\n",
        "- the pattern of the feature detector is detected using this process\n",
        "- various convolutions are performed with various feature detectors\n",
        "\n",
        "#### Step-1(b): reLU (rectifier linear unit)\n",
        "- a rectifier function function is applied to the output to increase the non-linearity\n",
        "\n",
        "## Step-2: Max-pooling (also called down-sampling)\n",
        "- when a cnn model is learning a particular feature and finds it a particular position in the image, it will try to look for this feature at the exact same position in all images\n",
        "- since, the feature will not be at the same position in all images, the model will turn out to be very inefficient\n",
        "- to avoid this, the model should be spatially invariant\n",
        "- for max-pooling, in the feature map obtained in the previous step, take 2x2 groups of cells and record only the maximum value in the group\n",
        "- do this for all adjacent groups\n",
        "- through this process, we get rid of 75% of the information which is not the feature\n",
        "- also we account for some amount of distortion, ie. if the maximum value in a certain group lied in any of the 4 cells, we'd still have it in the same position in the pooled image\n",
        "- hence if the image is rotated or squashed a bit, we can still detect it\n",
        "- it also helps in preventing over-fitting\n",
        "\n",
        "## Step-3: Flattening\n",
        "- the matrix is put row by row into a 1D array, this is called flattening\n",
        "- this is done to feed this data into an ANN later\n",
        "\n",
        "## Step-4: Full connection\n",
        "- this 1D array is fed into a fully connected ANN\n",
        "- from the outputs obtained from the ANN, the error is calculated using a loss function\n",
        "- the error is backpropagated\n",
        "- this error is used to adjust the weights in the ANN and also the feature detectors in the CNN\n",
        "\n",
        "## Softmax and Cross-Entropy\n",
        "- in the output layer, there can be any number of neurons\n",
        "- each neuron output can be any real number, which do not necessarily need to add up to 1\n",
        "- in order to make the values add up to 1, softmax function is used\n",
        "- hence all the output values are between 0 and 1\n",
        "- the cross-entropy function is the loss function which needs to be minimised during the training\n",
        "- cross-entropy is a better measure of error compared to mean squared error because in the inital stages of training and backpropagation, the output values are very low and hence the slope of gradient descent curve is also very low. MSE won't be able to properly detect it and start moving along the gradient. However, cross-entropy can actually detect this and help in moving forward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCV30xyVhFbE"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIleuCAjoFD8"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set\n",
        "- some transformations are applied to the training set to avoid overfitting\n",
        "- **rescale** is used to apply feature scaling, **shear_range** applies transvection, **zoom_range** is used to zoom the image, **horizontal_flip** is self-explanatory\n",
        "- **target_size** is to resize the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0koUcJMJpEBD"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set\n",
        "- transformations are not applied to test dataset ofcourse\n",
        "- only feature scaling is applied"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH4WzfOhpKc3"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAUt4UMPlhLS"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution\n",
        "- since the images have been resized to 64x64, so the input shape will be [64, 64, 3] where 3 stands for RGB colored images. If the images were black and white, this parameter would have been 1\n",
        "- kernel_size is the size of the feature detector matrix\n",
        "- filters stands for number of feature detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzPrMckl-hV"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncpqPl69mOac"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer\n",
        "- here input_shape is not required as it is connected to the previous layer, not the input layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-FZjn_m8gk"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZeOGCvnNZn"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GtmUlLd26Nq"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_Zj1Mc3Ko_"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALksrNQpUlJ"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUj1W4PJptta"
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction\n",
        "- during data preprocessing, an extra dimension of batch size was required\n",
        "- this needs to be added for any prediction we make\n",
        "- this is done using np.expand_dims with axis=0 which means this will be the first dimension\n",
        "- this is intuitive because first we load a batch of images and then process each image in that batch, hence batch size should be the first dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSiWEJY1BPB"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED9KB3I54c1i"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}